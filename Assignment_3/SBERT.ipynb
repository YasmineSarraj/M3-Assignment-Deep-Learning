{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# üöÄ  Applied Deep Learning\n",
    "\n",
    "**Topic:** Create a search enigne to look up clothes on Zalando using SBERT and semantic search. \n",
    "\n",
    "Things we can do: \n",
    "- Use images and CLIP\n",
    "- SetFit for supervised tasks with SBERT models \n",
    "\n",
    "Inspo: \n",
    "- GIF search engine\n",
    "- Youtube Search \n",
    "- get podcasts transcript for a specific topic (or create transcripts with Whisper - search for OpenAI Whisper Colab)\n",
    "- Finetune an SBERT model using domain adaptation \n",
    "- Embed and build a search engine \n",
    "- Build a Gradio App\n",
    "\n",
    "Nilso's idea: \n",
    "- downlod pictures from zalando and then encode them\n",
    "- And then the user can query, write a string which we encode and then we compare the cos sim distances between the pictures and show them ordered\n",
    "-> How the fuck do we scrape from Zalando + the search in Zalando is not too shitty. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "theory + exemple: \n",
    "https://www.sbert.net/examples/applications/image-search/README.html\n",
    "https://www.pinecone.io/learn/gif-search/\n",
    "https://medium.com/mlearning-ai/semantic-search-with-s-bert-is-all-you-need-951bc710e160\n",
    "https://www.youtube.com/watch?v=6yPWtdgs5Sg&ab_channel=code_your_own_AI\n",
    "https://github.com/sonynka/fashion_scraper\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies  & load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image\n",
    "import glob\n",
    "import torch\n",
    "import pickle\n",
    "import zipfile\n",
    "from IPython.display import display\n",
    "from IPython.display import Image as IPImage\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm\n",
    "torch.set_num_threads(4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Components\n",
    "CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3. We found CLIP matches the performance of the original ResNet50 on ImageNet ‚Äúzero-shot‚Äù without using any of the original 1.28M labeled examples, overcoming several major challenges in computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we load the respective CLIP model\n",
    "model = SentenceTransformer(\"clip-ViT-B-32-multilingual-v1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "Here we are trying to get some pictures from zalando that have both a picture and captions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we get about 25k images from Unsplash \n",
    "img_folder = 'photos/'\n",
    "if not os.path.exists(img_folder) or len(os.listdir(img_folder)) == 0:\n",
    "    os.makedirs(img_folder, exist_ok=True)\n",
    "    \n",
    "    photo_filename = 'unsplash-25k-photos.zip'\n",
    "    if not os.path.exists(photo_filename):   #Download dataset if does not exist\n",
    "        util.http_get('http://sbert.net/datasets/'+photo_filename, photo_filename)\n",
    "        \n",
    "    #Extract all images\n",
    "    with zipfile.ZipFile(photo_filename, 'r') as zf:\n",
    "        for member in tqdm(zf.infolist(), desc='Extracting'):\n",
    "            zf.extract(member, img_folder)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# Connect to pinecone environment\n",
    "pinecone.init(\n",
    "    api_key=\"YOUR_API_KEY\",\n",
    "    environment=\"YOUR_ENV\"  # find next to API key\n",
    ")\n",
    "\n",
    "index_name = 'gif-search'\n",
    "\n",
    "# check if the gif-search exists\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # create the index if it does not exist\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\"\n",
    "    )\n",
    "\n",
    "# Connect to gif-search index we created\n",
    "index = pinecone.Index(index_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6c8b6df6e8e73a4eab5f9bf87a5aa3461fae89bacc426305b809e1fd6049d9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
