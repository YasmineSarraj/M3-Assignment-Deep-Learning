{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêß  Applied Deep Learning\n",
    "\n",
    "**Topic:** Applied Deep Learning for Classification on the penguin dataset\n",
    "\n",
    "In this part we are preprocessing the data to feed it to the model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something fucks up with my environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter server crashed. Unable to connect. \n",
      "\u001b[1;31mError code from Jupyter: 1\n",
      "\u001b[1;31musage: jupyter.py [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "\u001b[1;31m                  [--paths] [--json] [--debug]\n",
      "\u001b[1;31m                  [subcommand]\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mJupyter: Interactive Computing\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mpositional arguments:\n",
      "\u001b[1;31m  subcommand     the subcommand to launch\n",
      "\u001b[1;31m\n",
      "\u001b[1;31moptional arguments:\n",
      "\u001b[1;31m  -h, --help     show this help message and exit\n",
      "\u001b[1;31m  --version      show the versions of core jupyter packages and exit\n",
      "\u001b[1;31m  --config-dir   show Jupyter config dir\n",
      "\u001b[1;31m  --data-dir     show Jupyter data dir\n",
      "\u001b[1;31m  --runtime-dir  show Jupyter runtime dir\n",
      "\u001b[1;31m  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "\u001b[1;31m                 format.\n",
      "\u001b[1;31m  --json         output paths as machine-readable json\n",
      "\u001b[1;31m  --debug        output debug information about paths\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mAvailable subcommands:\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mJupyter command `jupyter-notebook` not found. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# loading essential libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv(\"https://github.com/allisonhorst/palmerpenguins/raw/5b5891f01b52ae26ad8cb9755ec93672f49328a8/data/penguins_size.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide categorical and numerical data to encode and scale the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we scale the numerical variable\n",
    "num = penguins.iloc[:,2:6]\n",
    "scaler = StandardScaler()\n",
    "num = scaler.fit_transform(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We encode a categorical variable. \n",
    "cat = penguins.iloc[:, 0]\n",
    "labelencoder = LabelEncoder()\n",
    "cat = labelencoder.fit_transform(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensor of the categorical varibale \n",
    "X_tensor = torch.tensor(cat.astype(np.float32).values)\n",
    "\n",
    "#Tensor Of all the numerical values, lenghth etc. \n",
    "y_tensor = torch.tensor(num.astype(np.float32).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the type\n",
    "print(X_tensor.dtype)\n",
    "print(y_tensor.dtype)\n",
    "\n",
    "#Check the shape\n",
    "print(X_tensor.shape)\n",
    "print(y_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Hyperparameters\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "loss_set = {}\n",
    "\n",
    "# 1. Creating a FeedForwardNetwork\n",
    "model_net4 = torch.nn.Sequential(\n",
    "                         torch.nn.Linear(1,6),\n",
    "                         torch.nn.ReLU(),\n",
    "                         torch.nn.Linear(6,5),\n",
    "                         torch.nn.ReLU(),\n",
    "                         torch.nn.Linear(5,1),\n",
    "                         torch.nn.Sigmoid()\n",
    "                         );\n",
    "loss_mse = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.RMSprop(model_net4.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loop over the number of epochs\n",
    "for i in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n",
    "      lost_set_epoch = {}\n",
    "      j = 0\n",
    "      for x, y_t in zip(X_tensor, y_tensor):\n",
    "          output = model_net4.forward(x.float())\n",
    "          # 2. FeedForward Evaluation\n",
    "          loss = loss_mse(output, y_t.float())\n",
    "          optimizer.zero_grad();\n",
    "\n",
    "          # 3. Backward / Gradient Calculation\n",
    "          loss.backward()\n",
    "          # 4. Back Propagation\n",
    "          optimizer.step()\n",
    "          # Store the loss for each sample of data\n",
    "          lost_set_epoch[j] = loss\n",
    "          j = j + 1\n",
    "      \n",
    "      # Store the loss for each epoch\n",
    "      loss_set[i] = torch.mean(torch.stack(list(lost_set_epoch.values()))).detach().numpy()\n",
    "\n",
    "      # Display the loss after every 10 epochs\n",
    "      if (i % 10)==0:\n",
    "         print (f\"Loss: {loss_set[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(list(loss_set.values())).astype(float))\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
